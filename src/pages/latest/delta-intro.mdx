---
title: Introduction
description: Learn about Delta Lake features and resources to learn about Delta Lake.
menu: docs
---

[Delta Lake](https://delta.io) is an [open source
project](https://github.com/delta-io/delta) that enables building a [Lakehouse
architecture](https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html)
on top of [data lakes](https://databricks.com/discover/data-lakes/introduction).

Delta Lake provides [ACID transactions](concurrency-control.md), scalable
metadata handling, and unifies [streaming](delta-streaming.md) and
[batch](delta-batch.md) data processing on top of existing data lakes, such as
S3, ADLS, GCS, and HDFS.

Specifically, Delta Lake offers:

- [ACID transactions](/latest/concurrency-control) on Spark: Serializable isolation
  levels ensure that readers never see inconsistent data.
- Scalable metadata handling: Leverages Spark distributed processing power to
  handle all the metadata for petabyte-scale tables with billions of files at
  ease.
- [Streaming](/latest/delta-streaming) and [batch](/latest/delta-batch) unification: A
  table in Delta Lake is a batch table as well as a streaming source and sink.
  Streaming data ingest, batch historic backfill, interactive queries all just
  work out of the box.
- Schema enforcement: Automatically handles schema variations to prevent
  insertion of bad records during ingestion.
- [Time travel](/latest/delta-batch#deltatimetravel): Data versioning enables
  rollbacks, full historical audit trails, and reproducible machine learning
  experiments.
- [Upserts](/latest/delta-update#upsert-into-a-table-using-merge) and
  [deletes](/latest/delta-update#delete-from-a-table): Supports merge, update and delete
  operations to enable complex use cases like change-data-capture,
  slowly-changing-dimension (SCD) operations, streaming upserts, and so on.
